# Emergency Messages Classifier

## About this project

This project classifies emergency messages that were sent to and from emergency agencies like FEMA. The idea behind this is to make seeing what a message might be about without having to first read the whole message in context. In emergency situations messages sent be it from text, voice, etc. are all about speed. The longer it takes to quantify the content of a message the longer it takes for a responce to be made. The goal is to reduce the receipt to response time.

## Abstract

This project is a very simple machine learning classifier using TF-IDF and a few other components to classify text messages that were sent during emergency situations. These situations could be fires, floods, etc. I am attempting to classify the text messages based on 36 unique criteria.

Once classified I have a web app that you can use to write a unique message which is then processed by the same classifier. This will produce a result which will show boolean values for each category.

## Files

In the code base you'll see a few python files, a frontend folder, data folder, and pickles folder.

- The python files are used to store (in a database), train, process, and host the api for the web front-end
- The frontend folder is a node project which has a react frontend that simply interacts with the python API.
- The data folder holds the messages.csv and categories.csv files which hold the raw data we use to process these messages.
- The pickles folder holds the pickle binaries for the algorithms saved, for use with the API.

Here's a break down of the file tree.

```
ðŸ“¦data-science-proj2
 â”£ ðŸ“‚data #storage for datafiles
 â”ƒ â”£ ðŸ“œcategories.csv
 â”ƒ â”— ðŸ“œmessages.csv
 â”£ ðŸ“‚frontend #the front-end react app
 â”ƒ â”£ ðŸ“‚public #public files
 â”ƒ â”ƒ â”£ ðŸ“‚plots #the plot images
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œaid_centers.png
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œaid_centers_cv.png
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œaid_related.png
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œaid_related_cv.png
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œbuildings.png
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œbuildings_cv.png
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œchild_alone.png
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œchild_alone_cv.png
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œclothing.png
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œclothing_cv.png
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œcold.png
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œcold_cv.png
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œdeath.png
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œdeath_cv.png
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œdirect_report.png
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œdirect_report_cv.png
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œearthquake.png
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œearthquake_cv.png
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œelectricity.png
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œelectricity_cv.png
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œfire.png
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œfire_cv.png
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œfloods.png
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œfloods_cv.png
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œfood.png
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œfood_cv.png
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œhospitals.png
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œhospitals_cv.png
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œinfrastructure_related.png
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œinfrastructure_related_cv.png
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œmedical_help.png
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œmedical_help_cv.png
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œmedical_products.png
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œmedical_products_cv.png
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œmilitary.png
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œmilitary_cv.png
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œmissing_people.png
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œmissing_people_cv.png
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œmoney.png
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œmoney_cv.png
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œoffer.png
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œoffer_cv.png
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œother_aid.png
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œother_aid_cv.png
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œother_infrastructure.png
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œother_infrastructure_cv.png
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œother_weather.png
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œother_weather_cv.png
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œrefugees.png
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œrefugees_cv.png
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œrelated.png
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œrelated_cv.png
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œrequest.png
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œrequest_cv.png
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œsearch_and_rescue.png
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œsearch_and_rescue_cv.png
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œsecurity.png
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œsecurity_cv.png
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œshelter.png
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œshelter_cv.png
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œshops.png
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œshops_cv.png
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œstorm.png
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œstorm_cv.png
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œtools.png
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œtools_cv.png
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œtransport.png
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œtransport_cv.png
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œwater.png
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œwater_cv.png
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œweather_related.png
 â”ƒ â”ƒ â”ƒ â”— ðŸ“œweather_related_cv.png
 â”ƒ â”ƒ â”£ ðŸ“œfavicon.ico
 â”ƒ â”ƒ â”£ ðŸ“œindex.html #default html incase react doesnt load
 â”ƒ â”ƒ â”£ ðŸ“œlogo192.png
 â”ƒ â”ƒ â”£ ðŸ“œlogo512.png
 â”ƒ â”ƒ â”£ ðŸ“œmanifest.json
 â”ƒ â”ƒ â”— ðŸ“œrobots.txt #for search engines
 â”ƒ â”£ ðŸ“‚src
 â”ƒ â”ƒ â”£ ðŸ“œApp.css #css file, uses tailwind
 â”ƒ â”ƒ â”£ ðŸ“œApp.js #main app
 â”ƒ â”ƒ â”£ ðŸ“œApp.test.js #testing framework
 â”ƒ â”ƒ â”£ ðŸ“œindex.css #default file
 â”ƒ â”ƒ â”£ ðŸ“œindex.js #default js file
 â”ƒ â”ƒ â”£ ðŸ“œlogo.svg #the logo
 â”ƒ â”ƒ â”£ ðŸ“œreportWebVitals.js #heartbeat type info
 â”ƒ â”ƒ â”£ ðŸ“œsetupTests.js #run tests
 â”ƒ â”ƒ â”— ðŸ“œtable.js #the table component
 â”ƒ â”£ ðŸ“œ.gitignore #the gitignore file
 â”ƒ â”£ ðŸ“œpackage-lock.json #the lock file for packages
 â”ƒ â”£ ðŸ“œpackage.json #the raw package
 â”ƒ â”£ ðŸ“œpostcss.config.js #postcss config
 â”ƒ â”— ðŸ“œtailwind.config.js #tailwinds config file
 â”£ ðŸ“‚pickles
 â”ƒ â”— ðŸ“œ.keep #no pickles are included due to size
 â”£ ðŸ“œ.gitignore
 â”£ ðŸ“œMakefile #shortcuts
 â”£ ðŸ“œPipfile #the list of packages for python
 â”£ ðŸ“œPipfile.lock #the auth list of packages
 â”£ ðŸ“œProcfile #for remote server deployment
 â”£ ðŸ“œdevelopment.db #the database
 â”£ ðŸ“œnltk.txt #the nltk packages to download
 â”£ ðŸ“œprocess_data.py #process data here
 â”£ ðŸ“œreadme.md #readme markdown for github
 â”£ ðŸ“œrun.py #run the api
 â”— ðŸ“œtrain_classifier.py #train the classifier
```

## The Data

The data is split between two files one being and ID and message, the other being an ID and 36 categories. I have to ingest the files, split the categories, create a data frame of the merged data, and then save that data to a SQLite database for use by the ML script. The data is useful but incomplete, some categories have really bad numbers for training and other categories are simply unreliable.

Spending time on and doing additional work on the tokenizer more or working with another single to multi-out algorithm may result in better results. This is especially true if we start to ignore the useless or unreliable data. In a perfect world I would have much more data to work off of. More data, especially with categories that are underrepresented would make the results far better.

F1 scoring is all over due to the imbalanced data, recall on some categories is great, others require specific wording to get a category to flip. My focus was on the process, data, code, etc. not on the outcome of the algorithm. I believe this could be greatly improved at some point.

## Screenshots of the App

<img width="1515" alt="image" src="https://user-images.githubusercontent.com/55346/168404777-9a701805-09da-4ed6-9b62-46b5164d0fab.png">
<img width="1516" alt="image" src="https://user-images.githubusercontent.com/55346/168404787-555bc5b8-930c-4979-9a99-965447fc9ff0.png">
<img width="699" alt="image" src="https://user-images.githubusercontent.com/55346/168404802-344fedb2-a51a-406e-bc6a-f8801ed12ba0.png">

## Running the Code

The app needs the following steps done to work properly. You will need Python3 and Node > 10 to run this code.

1. Process the raw data using `pipenv run python process_data.py` file directly or by calling `make process`.
2. Train the ML algorithm by running the `pipenv run python train_classifier.py` file directly or by calling `make train`.
3. Run the API backend using the `pipenv run python run.py` file directly or by calling `make api`.
4. While the API is running, `CD into the ./frontend` folder and run `npm run start` file directly or by calling `make web` from the root.

Convenience Methods have been provided using a `Makefile`
| Command | Action |
|--|--|
| make setup | Installs Pipfile, package.json and setups up env for python and node. |
| make process | Runs the process_data.py script in the pipenv. |
| make train | Runs the train_classifier.py script in the pipenv. |
| make api | Runs the API backend via run.py. |
| make web | This will cd into frontend and start the react client. |

If you run the commands in the order above, you should have a fully functional Web front-end and trained ML classifier available at [http://localhost:3000](http://localhost:3000)
